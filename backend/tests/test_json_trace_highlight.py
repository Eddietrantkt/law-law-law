"""
Test: JSON Data Traceability with Text Highlighting
Ki·ªÉm tra kh·∫£ nƒÉng truy ng∆∞·ª£c t·ª´ search results v·ªÅ v·ªã tr√≠ trong file JSON g·ªëc
v√† highlight ƒëo·∫°n text ƒë∆∞·ª£c tr√≠ch d·∫´n

Workflow:
1. User ƒë·∫∑t c√¢u h·ªèi
2. System search trong FAISS/BM25 ‚Üí tr·∫£ v·ªÅ chunks
3. M·ªói chunk c√≥ metadata: source file, ƒëi·ªÅu, kho·∫£n
4. Load l·∫°i file JSON g·ªëc
5. T√¨m v√† highlight ƒëo·∫°n text ƒë∆∞·ª£c tr√≠ch d·∫´n
6. Hi·ªÉn th·ªã context xung quanh v·ªõi highlight

Ch·∫°y: python test_json_trace_highlight.py
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import json
import glob
from termcolor import colored, cprint
from sentence_transformers import SentenceTransformer
import google.generativeai as genai
from dotenv import load_dotenv

# Import core functions
from core.document_processor import xu_ly_van_ban_phap_luat_json
from core.search import advanced_hybrid_search
from core.intent_detection import enhanced_decompose_query
from utils.cache import build_or_load_bm25, build_or_load_faiss, get_data_hash
from utils.tokenizer import tokenize_vi


def highlight_text_in_context(full_text, search_text, context_chars=200):
    """
    T√¨m v√† highlight ƒëo·∫°n text trong context
    
    Args:
        full_text: Text ƒë·∫ßy ƒë·ªß
        search_text: Text c·∫ßn highlight
        context_chars: S·ªë k√Ω t·ª± context tr∆∞·ªõc/sau
        
    Returns:
        String v·ªõi highlight (s·ª≠ d·ª•ng ANSI colors)
    """
    # Normalize ƒë·ªÉ t√¨m ki·∫øm
    full_lower = full_text.lower()
    search_lower = search_text.lower()
    
    # T√¨m v·ªã tr√≠
    pos = full_lower.find(search_lower)
    
    if pos == -1:
        # Kh√¥ng t√¨m th·∫•y ch√≠nh x√°c, th·ª≠ t√¨m partial match
        # L·∫•y 50 k√Ω t·ª± ƒë·∫ßu c·ªßa search_text
        search_partial = search_lower[:50]
        pos = full_lower.find(search_partial)
    
    if pos == -1:
        return f"‚ö†Ô∏è  Text kh√¥ng t√¨m th·∫•y trong JSON\n{full_text[:300]}..."
    
    # L·∫•y context tr∆∞·ªõc v√† sau
    start = max(0, pos - context_chars)
    end = min(len(full_text), pos + len(search_text) + context_chars)
    
    # Extract c√°c ph·∫ßn
    before = full_text[start:pos]
    matched = full_text[pos:pos + len(search_text)]
    after = full_text[pos + len(search_text):end]
    
    # Add ellipsis n·∫øu c·∫ßn
    if start > 0:
        before = "..." + before
    if end < len(full_text):
        after = after + "..."
    
    # Format v·ªõi colors
    result = (
        colored(before, 'white', attrs=['dark']) +
        colored(matched, 'yellow', 'on_red', attrs=['bold']) +
        colored(after, 'white', attrs=['dark'])
    )
    
    return result


def trace_chunk_to_json(chunk, json_files_dict):
    """
    Truy ng∆∞·ª£c chunk v·ªÅ v·ªã tr√≠ trong file JSON g·ªëc
    
    Args:
        chunk: Search result chunk
        json_files_dict: Dict mapping filename -> JSON data
        
    Returns:
        Dict v·ªõi th√¥ng tin trace v√† highlighted text
    """
    source = chunk.get('source', '')
    content = chunk.get('content', '')
    metadata = chunk.get('metadata', {})
    
    # Get JSON filename from metadata (added in document_processor)
    json_filename = metadata.get('json_file', '')
    
    # Fallback: try to extract from source or match by law_name
    if not json_filename or json_filename not in json_files_dict:
        # Try to match by filename pattern in source
        for key in json_files_dict.keys():
            # Remove extension and check if in source
            key_base = key.replace('.json', '').replace('_hopnhat', '')
            if key_base in source.lower() or key in source:
                json_filename = key
                break
        
        # If still not found, try to match by law name
        if not json_filename or json_filename not in json_files_dict:
            law_name = metadata.get('law_name', '')
            # Create mapping based on common patterns
            law_to_json = {
                'Lu·∫≠t H√¥n nh√¢n': 'luat_hon_nhan_hopnhat.json',
                'Lu·∫≠t ƒê·∫•t ƒëai': 'luat_dat_dai_hopnhat.json',
                'Lu·∫≠t Lao ƒë·ªông': 'luat_lao_donghopnhat.json',
                'Lu·∫≠t ƒê·∫•u th·∫ßu': 'luat_dauthau_hopnhat.json',
                'Chuy·ªÉn giao c√¥ng ngh·ªá': 'chuyen_giao_cong_nghe_hopnhat.json',
                'Ngh·ªã ƒë·ªãnh 214': 'nghi_dinh_214_2025.json',
            }
            
            for pattern, filename in law_to_json.items():
                if pattern.lower() in law_name.lower():
                    json_filename = filename
                    break
    
    if not json_filename or json_filename not in json_files_dict:
        return {
            'status': 'error',
            'message': f'Cannot find JSON file. Source: {source[:100]}..., Law: {metadata.get("law_name", "N/A")}'
        }
    
    json_data = json_files_dict[json_filename]
    
    # Get PDF filename mapping
    json_to_pdf = {
        'luat_hon_nhan_hopnhat.json': 'luat_hon_nhan.pdf',
        'luat_dat_dai_hopnhat.json': 'luat_dat_dai.pdf',
        'luat_lao_donghopnhat.json': 'luat_lao_dong.pdf',
        'luat_dauthau_hopnhat.json': 'luat_dau_thau.pdf',
        'chuyen_giao_cong_nghe_hopnhat.json': 'luat_chuyen_giao_cong_nghe.pdf',
        'nghi_dinh_214_2025.json': 'nghi_dinh_214_2025.pdf',
    }
    
    pdf_filename = json_to_pdf.get(json_filename, 'UNKNOWN.pdf')
    
    # T√¨m article trong JSON
    article_num = metadata.get('article_num', '')
    
    if not article_num or 'du_lieu' not in json_data:
        return {
            'status': 'error',
            'message': 'Missing article number or invalid JSON structure'
        }
    
    # Search for article
    found_article = None
    for article in json_data['du_lieu']:
        if article.get('dieu_so') == str(article_num):
            found_article = article
            break
    
    if not found_article:
        return {
            'status': 'error',
            'message': f'Article {article_num} not found in JSON'
        }
    
    # Build full text c·ªßa article ƒë·ªÉ highlight
    article_text_parts = []
    
    # Ti√™u ƒë·ªÅ
    if found_article.get('tieu_de'):
        article_text_parts.append(found_article['tieu_de'])
    
    # M√¥ t·∫£
    if found_article.get('mo_ta'):
        article_text_parts.append(found_article['mo_ta'])
    
    # C√°c kho·∫£n
    if found_article.get('khoan'):
        for khoan in found_article['khoan']:
            if khoan.get('noi_dung'):
                article_text_parts.append(khoan['noi_dung'])
            
            # C√°c ƒëi·ªÉm
            if khoan.get('diem'):
                for diem in khoan['diem']:
                    if diem.get('noi_dung'):
                        article_text_parts.append(diem['noi_dung'])
    
    full_article_text = '\n'.join(article_text_parts)
    
    # Highlight content trong full text
    highlighted = highlight_text_in_context(
        full_article_text, 
        content[:200],  # Ch·ªâ d√πng 200 k√Ω t·ª± ƒë·∫ßu ƒë·ªÉ search
        context_chars=150
    )
    
    return {
        'status': 'success',
        'json_file': json_filename,
        'pdf_file': pdf_filename,
        'article_num': article_num,
        'article_title': found_article.get('tieu_de', ''),
        'full_text': full_article_text,
        'highlighted': highlighted,
        'metadata': metadata
    }


def test_json_traceability():
    """Test ch√≠nh: Truy ng∆∞·ª£c v√† highlight"""
    
    print("\n" + "="*80)
    cprint("JSON TRACEABILITY TEST WITH HIGHLIGHTING", 'cyan', attrs=['bold'])
    print("="*80)
    
    # 1. Load JSON files
    print("\nüìÇ Step 1: Loading JSON files...")
    json_files = glob.glob("data/*.json")
    
    if not json_files:
        cprint("‚ùå No JSON files found in data/", 'red')
        return
    
    # Load all JSON data v√†o memory
    json_files_dict = {}
    for json_file in json_files:
        filename = os.path.basename(json_file)
        with open(json_file, 'r', encoding='utf-8') as f:
            json_files_dict[filename] = json.load(f)
    
    cprint(f"‚úÖ Loaded {len(json_files_dict)} JSON files", 'green')
    
    # 2. Process to chunks
    print("\nüìä Step 2: Processing JSON to chunks...")
    all_chunks = []
    for json_file in json_files:
        chunks, law_source = xu_ly_van_ban_phap_luat_json(json_file)
        all_chunks.extend(chunks)
    cprint(f"‚úÖ Created {len(all_chunks)} chunks", 'green')
    
    # Sample chunk structure
    if all_chunks:
        sample = all_chunks[0]
        print(f"\n   Sample chunk:")
        print(f"   - source: {sample['source']}")
        print(f"   - metadata: {sample.get('metadata', {})}")
    
    # 3. Build search indexes
    print("\nüîß Step 3: Building search indexes...")
    
    # Initialize Gemini
    load_dotenv()
    api_key = os.getenv('GOOGLE_API_KEY')
    if api_key:
        genai.configure(api_key=api_key)
        gemini_lite_model = genai.GenerativeModel('gemini-2.5-flash-lite')
    else:
        cprint("‚ö†Ô∏è  No GOOGLE_API_KEY found, using simple search", 'yellow')
        gemini_lite_model = None
    
    # Initialize embedder
    embedder = SentenceTransformer('keepitreal/vietnamese-sbert')
    
    # Get data hash
    data_hash = get_data_hash(all_chunks)
    
    # Build BM25 index (need tokenized corpus)
    corpus = [tokenize_vi(chunk['content']) for chunk in all_chunks]
    bm25_index = build_or_load_bm25(corpus, data_hash)
    
    # Build FAISS index
    faiss_index, _ = build_or_load_faiss(all_chunks, data_hash, embedder)
    
    cprint("‚úÖ Indexes ready", 'green')
    
    # 4. Test queries
    test_queries = [
        "ƒê·ªô tu·ªïi k·∫øt h√¥n l√† bao nhi√™u?",
        "Quy·ªÅn s·ª≠ d·ª•ng ƒë·∫•t?",
        "ƒêi·ªÅu ki·ªán ƒë·∫•u th·∫ßu?"
    ]
    
    # Test v·ªõi query ƒë·∫ßu ti√™n
    query = test_queries[0]
    
    print("\n" + "="*80)
    cprint(f"üîç TESTING QUERY: '{query}'", 'cyan', attrs=['bold'])
    print("="*80)
    
    # 5. Search
    print("\n‚ö° Step 4: Searching...")
    
    # Create a wrapper function for enhanced_decompose_query
    def decompose_fn(query):
        if gemini_lite_model:
            return enhanced_decompose_query(query, gemini_lite_model)
        else:
            # Fallback: return simple result without LLM
            return {
                'is_legal': True,
                'confidence': 0.8,
                'sub_queries': [query],
                'keywords': []
            }
    
    results = advanced_hybrid_search(
        query=query,
        all_chunks=all_chunks,
        bm25_index=bm25_index,
        faiss_index=faiss_index,
        embedder=embedder,
        tokenize_fn=tokenize_vi,
        enhanced_decompose_fn=decompose_fn,
        top_k=3
    )
    
    cprint(f"‚úÖ Found {len(results)} relevant chunks", 'green')
    
    # 6. Trace v√† highlight t·ª´ng result
    print("\n" + "="*80)
    cprint("üìç TRACING RESULTS TO JSON SOURCE WITH HIGHLIGHTING", 'cyan', attrs=['bold'])
    print("="*80)
    
    for i, chunk in enumerate(results, 1):
        print("\n" + "-"*80)
        cprint(f"Result #{i}", 'yellow', attrs=['bold'])
        print("-"*80)
        
        # Basic info
        print(f"\nüìã Chunk Info:")
        print(f"   Source: {chunk['source']}")
        print(f"   Content preview: {chunk['content'][:100]}...")
        
        metadata = chunk.get('metadata', {})
        if metadata:
            print(f"\nüè∑Ô∏è  Metadata:")
            print(f"   Law: {metadata.get('law_name', 'N/A')}")
            print(f"   Article: ƒêi·ªÅu {metadata.get('article_num', 'N/A')}")
            print(f"   Clause: Kho·∫£n {metadata.get('clause_num', 'N/A')}")
        
        # Trace to JSON
        print(f"\nüîç Tracing to JSON source...")
        trace_result = trace_chunk_to_json(chunk, json_files_dict)
        
        if trace_result['status'] == 'error':
            cprint(f"   ‚ùå {trace_result['message']}", 'red')
            continue
        
        # Show trace result
        cprint(f"   ‚úÖ Found in: {trace_result['json_file']}", 'green')
        cprint(f"   üìÑ PDF file: {trace_result['pdf_file']}", 'cyan')
        print(f"   üìñ Article: {trace_result['article_title']}")
        
        # Show highlighted text
        print(f"\nüí° Highlighted in JSON:")
        print(f"   (Yellow text = matched content from search)")
        print()
        print(trace_result['highlighted'])
        print()
        
        # Show full article structure (optional)
        print(f"\nüìñ Full Article Structure:")
        print(f"   File: {trace_result['json_file']}")
        print(f"   ƒêi·ªÅu: {trace_result['article_num']}")
        print(f"   Total chars: {len(trace_result['full_text'])}")
    
    # 7. Summary
    print("\n" + "="*80)
    cprint("‚úÖ TEST COMPLETED", 'green', attrs=['bold'])
    print("="*80)
    
    print("\nüìä Summary:")
    print(f"   - Query: '{query}'")
    print(f"   - Results: {len(results)} chunks")
    print(f"   - Successfully traced: {sum(1 for r in results if trace_chunk_to_json(r, json_files_dict)['status'] == 'success')}")
    
    print("\nüí° What we proved:")
    cprint("   ‚úÖ Can trace from search results back to JSON source", 'green')
    cprint("   ‚úÖ Can find exact article in original JSON", 'green')
    cprint("   ‚úÖ Can highlight matched text with context", 'green')
    
    print("\nüéØ Next steps:")
    print("   - Implement this in frontend UI")
    print("   - Show JSON structure with highlighted citations")
    print("   - Allow users to see original document context")


def main():
    """Entry point"""
    try:
        # Check if termcolor is available
        try:
            import termcolor
        except ImportError:
            print("‚ö†Ô∏è  Installing termcolor for better output...")
            os.system("pip install termcolor --quiet")
            import termcolor
        
        # Run test
        test_json_traceability()
        
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Test interrupted by user")
    except Exception as e:
        print(f"\n\n‚ùå ERROR: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
